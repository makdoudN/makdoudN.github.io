{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2', 'created_at': '2024-11-15T14:36:50.442168Z', 'response': '{\\n  \"introduction\": \"Regular physical activity is a fundamental aspect of a healthy lifestyle, offering numerous benefits for overall well-being and quality of life.\",\\n  \"main_points\": [\\n    \"Point 1: Improved Mental Health - Exercise has been shown to reduce symptoms of anxiety and depression, while also improving mood and cognitive function.\",\\n    \"Point 2: Enhanced Physical Health - Regular physical activity can help maintain a healthy weight, lower blood pressure, and increase strength and flexibility.\",\\n    \"Point 3: Increased Energy and Productivity - Exercise boosts energy levels, enhances sleep quality, and improves focus and concentration, leading to increased productivity and better work performance.\"\\n  ],\\n  \"conclusion\": \"Incorporating exercise into one\\'s daily routine can lead to a significant improvement in overall health and well-being, resulting in a longer, healthier, and happier life.\"\\n}', 'done': True, 'done_reason': 'stop', 'context': [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 1432, 262, 5321, 1833, 420, 21782, 304, 701, 2077, 512, 262, 341, 220, 330, 396, 17158, 794, 330, 87084, 24131, 761, 220, 330, 3902, 13224, 794, 2330, 262, 330, 2674, 220, 16, 761, 262, 330, 2674, 220, 17, 761, 262, 330, 2674, 220, 18, 702, 220, 3291, 220, 330, 444, 9134, 794, 330, 19791, 702, 534, 1084, 262, 60601, 25, 83017, 279, 7720, 315, 10368, 198, 257, 128009, 128006, 78191, 128007, 271, 517, 220, 330, 396, 17158, 794, 330, 31504, 7106, 5820, 374, 264, 16188, 13189, 315, 264, 9498, 19433, 11, 10209, 12387, 7720, 369, 8244, 1664, 33851, 323, 4367, 315, 2324, 10560, 220, 330, 3902, 13224, 794, 2330, 262, 330, 2674, 220, 16, 25, 59223, 38895, 6401, 482, 33918, 706, 1027, 6982, 311, 8108, 13803, 315, 18547, 323, 18710, 11, 1418, 1101, 18899, 20247, 323, 25702, 734, 10560, 262, 330, 2674, 220, 17, 25, 62549, 28479, 6401, 482, 29900, 7106, 5820, 649, 1520, 10519, 264, 9498, 4785, 11, 4827, 6680, 7410, 11, 323, 5376, 8333, 323, 25152, 10560, 262, 330, 2674, 220, 18, 25, 62697, 12634, 323, 5761, 1968, 482, 33918, 67232, 4907, 5990, 11, 57924, 6212, 4367, 11, 323, 36050, 5357, 323, 20545, 11, 6522, 311, 7319, 26206, 323, 2731, 990, 5178, 10246, 220, 3291, 220, 330, 444, 9134, 794, 330, 644, 6133, 4406, 1113, 10368, 1139, 832, 596, 7446, 14348, 649, 3063, 311, 264, 5199, 16048, 304, 8244, 2890, 323, 1664, 33851, 11, 13239, 304, 264, 5129, 11, 39345, 11, 323, 44467, 2324, 10246, 92], 'total_duration': 5510500750, 'load_duration': 820711625, 'prompt_eval_count': 91, 'prompt_eval_duration': 1284000000, 'eval_count': 177, 'eval_duration': 3401000000}\n",
      "{\n",
      "  \"introduction\": \"Regular physical activity is a fundamental aspect of a healthy lifestyle, offering numerous benefits for overall well-being and quality of life.\",\n",
      "  \"main_points\": [\n",
      "    \"Point 1: Improved Mental Health - Exercise has been shown to reduce symptoms of anxiety and depression, while also improving mood and cognitive function.\",\n",
      "    \"Point 2: Enhanced Physical Health - Regular physical activity can help maintain a healthy weight, lower blood pressure, and increase strength and flexibility.\",\n",
      "    \"Point 3: Increased Energy and Productivity - Exercise boosts energy levels, enhances sleep quality, and improves focus and concentration, leading to increased productivity and better work performance.\"\n",
      "  ],\n",
      "  \"conclusion\": \"Incorporating exercise into one's daily routine can lead to a significant improvement in overall health and well-being, resulting in a longer, healthier, and happier life.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_with_outline(prompt, outline, model=\"llama3.2\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    formatted_prompt = f\"\"\"\n",
    "    Please follow this outline in your response:\n",
    "    {json.dumps(outline, indent=2)}\n",
    "    \n",
    "    Prompt: {prompt}\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=payload)\n",
    "    print(response.json())\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "# Example usage\n",
    "outline = {\n",
    "    \"introduction\": \"Brief overview\",\n",
    "    \"main_points\": [\n",
    "        \"Point 1\",\n",
    "        \"Point 2\",\n",
    "        \"Point 3\"\n",
    "    ],\n",
    "    \"conclusion\": \"Summary\"\n",
    "}\n",
    "\n",
    "result = generate_with_outline(\n",
    "    prompt=\"Explain the benefits of exercise\",\n",
    "    outline=outline\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import outlines \n",
    "\n",
    "class Hobby(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    duration: int = Field(..., description=\"Years since having hobby\")\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name:str\n",
    "    birth_year: int = Field(..., ge=1990, le=2025)\n",
    "    hobbies: List[Hobby]\n",
    "    alive: bool\n",
    "\n",
    "\n",
    "lm = outlines.models.openai(\"llama3.2\", base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlines import models, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n.makdoud/miniforge3/envs/llm/lib/python3.11/site-packages/outlines/models/llamacpp.py:391: UserWarning: The pre-tokenizer in `llama.cpp` handles unicode improperly (https://github.com/ggerganov/llama.cpp/pull/5613)\n",
      "Outlines may raise a `RuntimeError` when building the regex index.\n",
      "To circumvent this error when using `models.llamacpp()` you may pass the argument`tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(<hf_repo_id>)`\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827e01967e9744b09f315319fc64ce77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phi-3-mini-4k-instruct-q4.gguf:   0%|          | 0.00/2.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.llamacpp(\"microsoft/Phi-3-mini-4k-instruct-gguf\", \"Phi-3-mini-4k-instruct-q4.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generate.text(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d35379be7fc47058de6abb927f5741d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "qwen2-0_5b-instruct-q8_0.gguf:   0%|          | 0.00/531M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 290 tensors from /Users/n.makdoud/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct-GGUF/snapshots/198f08841147e5196a6a69bd0053690fb1fd3857/./qwen2-0_5b-instruct-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = qwen2-0_5b-instruct\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = ../Qwen2/gguf/qwen2-0_5b-imatrix/imat...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = ../sft_2406.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 168\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 1937\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type q8_0:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 293\n",
      "llm_load_vocab: token to piece cache size = 0.9338 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 896\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 14\n",
      "llm_load_print_meta: n_head_kv        = 2\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 128\n",
      "llm_load_print_meta: n_embd_v_gqa     = 128\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 494.03 M\n",
      "llm_load_print_meta: model size       = 500.79 MiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = qwen2-0_5b-instruct\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/25 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   500.79 MiB\n",
      "...........................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =     6.00 MiB\n",
      "llama_new_context_with_model: KV self size  =    6.00 MiB, K (f16):    3.00 MiB, V (f16):    3.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   298.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 846\n",
      "llama_new_context_with_model: graph splits = 386\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '168', 'general.quantization_version': '2', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'quantize.imatrix.file': '../Qwen2/gguf/qwen2-0_5b-imatrix/imatrix.dat', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.pre': 'qwen2', 'quantize.imatrix.chunks_count': '1937', 'tokenizer.ggml.model': 'gpt2', 'general.file_type': '7', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.embedding_length': '896', 'qwen2.attention.head_count_kv': '2', 'qwen2.context_length': '32768', 'quantize.imatrix.dataset': '../sft_2406.txt', 'qwen2.attention.head_count': '14', 'general.architecture': 'qwen2', 'qwen2.block_count': '24', 'qwen2.feed_forward_length': '4864', 'general.name': 'qwen2-0_5b-instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "    filename=\"*q8_0.gguf\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     450.31 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    29 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2164.19 ms /   241 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-f58d5778-37e4-4a42-ac00-2771865e5bf5',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1731683163,\n",
       " 'model': '/Users/n.makdoud/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct-GGUF/snapshots/198f08841147e5196a6a69bd0053690fb1fd3857/./qwen2-0_5b-instruct-q8_0.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': \"As an AI language model, I don't have access to specific images to describe. However, I can describe the general characteristics of an image based on its type, medium, and subject.\\n\\nFor example, if an image is a landscape photograph taken in a cityscape, the general characteristics may include:\\n\\n- The subject is a person or object in the foreground, such as a person walking down the street or a building.\\n- The scene is typically bright and sunny, with natural elements like trees and buildings visible.\\n- The sky, clouds, and other elements in the sky are often used to add depth to the image.\\n- The subject's clothing or accessories are typically well-suited to the scene, reflecting their style and mood.\\n\\nIf an image is a portrait, the subject may be a person or object that the photographer intended to capture. The portrait may be taken in natural light with the subject looking at the photographer with eyes wide open, revealing their emotional state and personality.\\n\\nOverall, the key characteristics of any image will depend on its intended purpose and subject.\"},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 29, 'completion_tokens': 212, 'total_tokens': 241}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.create_chat_completion(\n",
    "      messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images.\"},\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Describe this image in detail please.\"\n",
    "          }\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Harry Potter\",\n",
      "  \"age\": 11,\n",
      "  \"fact\": [\n",
      "    \"Wears number 7.\",\n",
      "    \"Can perform a perfect wand movement.\",\n",
      "    \"Enjoys treacle fudge and pumpkin pasties.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "import instructor\n",
    "\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    fact: List[str] = Field(..., description=\"A list of facts about the character\")\n",
    "\n",
    "\n",
    "# enables `response_model` in create call\n",
    "client = instructor.from_openai(\n",
    "    OpenAI(\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\",  # required, but unused\n",
    "    ),\n",
    "    mode=instructor.Mode.JSON,\n",
    ")\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me about the Harry Potter\",\n",
    "        }\n",
    "    ],\n",
    "    response_model=Character,\n",
    ")\n",
    "print(resp.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat(auth): Add OAuth2 login support\n",
      "\n",
      "This change integrates OAuth2 login with Google and Facebook.\n",
      "\n",
      "BREAKING CHANGE: Updated login API; old tokens are invalidated.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class ConventionalCommit(BaseModel):\n",
    "    type: str = Field(\n",
    "        ...,\n",
    "        description=\"The type of change (e.g., feat, fix, chore, refactor).\",\n",
    "        pattern=\"^(feat|fix|chore|refactor|test|docs|style|perf|ci|build|revert)$\",\n",
    "    )\n",
    "    scope: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"The scope of the change (e.g., a specific module or component).\"\n",
    "    )\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"A short description of the change.\",\n",
    "        max_length=72\n",
    "    )\n",
    "    body: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Detailed explanation of the change.\"\n",
    "    )\n",
    "    footer: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Additional information (e.g., breaking changes or issues fixed).\"\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def formatted(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the conventional commit in the correct format.\n",
    "        \"\"\"\n",
    "        parts = [f\"{self.type}{f'({self.scope})' if self.scope else ''}: {self.description}\"]\n",
    "        if self.body:\n",
    "            parts.append(f\"\\n\\n{self.body}\")\n",
    "        if self.footer:\n",
    "            parts.append(f\"\\n\\n{self.footer}\")\n",
    "        return \"\".join(parts)\n",
    "\n",
    "# Example usage\n",
    "example_commit = ConventionalCommit(\n",
    "    type=\"feat\",\n",
    "    scope=\"auth\",\n",
    "    description=\"Add OAuth2 login support\",\n",
    "    body=\"This change integrates OAuth2 login with Google and Facebook.\",\n",
    "    footer=\"BREAKING CHANGE: Updated login API; old tokens are invalidated.\"\n",
    ")\n",
    "\n",
    "print(example_commit.formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
