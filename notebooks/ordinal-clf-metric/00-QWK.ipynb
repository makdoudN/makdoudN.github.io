{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Cohen's Kappa Score measures** the level of agreement between two raters (or classifiers) who are assigning categorical labels to a set of items, while accounting for the agreement that would be expected by chance.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Let's say we have:\n",
    "- A set of $n$ items.\n",
    "- Two raters (or classifiers) assigning each item to one of $k$ possible categories.\n",
    "\n",
    "Define:\n",
    "- $O=$ observed agreement = proportion of cases where the two raters agree.\n",
    "- $E=$ expected agreement $=$ proportion of agreement that would be expected by random chance.\n",
    "\n",
    "Goal:\n",
    "We want a measure that tells us:\n",
    "- How much better the agreement is compared to random chance.\n",
    "\n",
    "The observed agreement is simply the proportion of cases where the two raters agree:\n",
    "\n",
    "$$\n",
    "O=\\frac{\\text { Number of agreements }}{n}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n=$ total number of items.\n",
    "- Number of agreements = number of items where both raters assign the same category.\n",
    "\n",
    "### **Expected Agreement by Chance**\n",
    "\n",
    "The expected agreement $E$ is computed based on the marginal probabilities that each rater assigns to a category.\n",
    "\n",
    "Let:\n",
    "- $p_i^A=$ proportion of items that Rater A assigns to category $i$.\n",
    "- $p_i^B=$ proportion of items that Rater B assigns to category $i$.\n",
    "\n",
    "The chance agreement for category $i$ is $p_i^A \\cdot p_i^B$ (because if both raters are choosing randomly, the probability they both pick the same category $i$ is the product of their independent probabilities).\n",
    "\n",
    "Thus, the total expected agreement is:\n",
    "\n",
    "$$\n",
    "E=\\sum_{i=1}^k p_i^A \\cdot p_i^B\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $k=$ number of categories.\n",
    "\n",
    "### **Cohen's Kappa Formula**\n",
    "\n",
    "Cohen's Kappa measures how much better the agreement is compared to random chance, scaled between -1 and 1 :\n",
    "\n",
    "$$\n",
    "\\kappa=\\frac{O-E}{1-E}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $O=$ observed agreement.\n",
    "- $E=$ expected agreement by chance.\n",
    "\n",
    "Interpretation:\n",
    "- $\\kappa=1 \\rightarrow$ Perfect agreement.\n",
    "- $\\kappa=0 \\rightarrow$ Agreement is no better than chance.\n",
    "- $\\kappa<0 \\rightarrow$ Worse than chance (systematic disagreement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6875)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "y1 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n",
    "y2 = [\"negative\", \"positive\", \"negative\", \"neutral\", \"negative\"]\n",
    "cohen_kappa_score(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y1, y2, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
